<?xml version="1.0"?>
<!DOCTYPE refentry PUBLIC "-//OASIS//DTD DocBook XML V4.5//EN" "http://www.oasis-open.org/docbook/xml/4.5/docbookx.dtd">
<refentry id="re-hbreport">
  <refentryinfo>
    <date>November 28, 2009</date>
    <productname>Cluster Glue</productname>
    <productnumber>@VERSION@</productnumber>
    <authorgroup>
      <author>
	<firstname>Dejan</firstname>
	<surname>Muhamedagic</surname>
	<contrib>man page</contrib>
	<email>dejan.muhamedagic@linbit.com</email>
      </author>
      <author>
	<firstname>Florian</firstname>
	<surname>Haas</surname>
	<contrib>man page</contrib>
	<email>florian.haas@linbit.com</email>
      </author>
    </authorgroup>
  </refentryinfo>
  <refmeta>
    <refentrytitle>hb_report</refentrytitle>
    <manvolnum>8</manvolnum>
    <refmiscinfo class="manual">System administration utilities</refmiscinfo>
  </refmeta>
  <refnamediv>
    <refname>hb_report</refname>
    <refpurpose>create report for Pacemaker clusters</refpurpose>
  </refnamediv>
  <refsynopsisdiv>
    <para><command>hb_report</command> <option>-f</option>&nbsp;(<replaceable>time</replaceable>|<token>cts:</token><replaceable>testnum</replaceable> [<option>-t</option>&nbsp;<replaceable>time</replaceable>] [<option>-u</option>&nbsp;<replaceable>user</replaceable>] [<option>-n</option>&nbsp;<replaceable>nodes</replaceable>] [<option>-E</option>&nbsp;<replaceable>files</replaceable>] [<option>-L</option>&nbsp;<replaceable>log-pattern</replaceable>] [<option>-p</option>&nbsp;<replaceable>variable-pattern</replaceable>] [<option>-e</option>&nbsp;<replaceable>prog</replaceable>] [<option>-MSDCAv</option>] <replaceable>destination</replaceable> </para>
  </refsynopsisdiv>
  <refsection id="rs-hbreport-description">
    <title>Description</title>
    <para><command>hb_report</command> is a utility to collect all
    information (logs, configuration files, system information, etc)
    relevant to Pacemaker (CRM) over the given period of time.</para>
  </refsection>
  <refsection id="rs-hbreport-options">
    <title>Options</title>
    <para>The following options are supported:</para>
    <variablelist>
      <varlistentry>
	<term>
	  <replaceable>dest</replaceable>
	</term>
	<listitem>
	  <para>The destination directory. Must be an absolute
	  path. The resulting tarball is placed in the parent
	  directory and contains the last directory element of this
	  path. Typically something like
	  <filename>/tmp/standby-failed</filename>.</para>
	</listitem>
      </varlistentry>
      <varlistentry>
	<term>
	  <option>-f</option>&nbsp;(<replaceable>time</replaceable>|<token>cts:</token><replaceable>testnum</replaceable>)
	</term>
	<listitem>
	  <para>The start time from which to collect logs. The time is
	  in the format as used by the Date::Parse perl module. For
	  cts tests, specify the "cts:" string followed by the test
	  number. This option is required.</para>
	</listitem>
      </varlistentry>
      <varlistentry>
	<term>
	  <option>-t</option>
	</term>
	<listitem>
	  <para>The end time to which to collect logs. Defaults to
	  now.</para>
	</listitem>
      </varlistentry>
      <varlistentry>
	<term>
	  <option>-n</option>&nbsp;<replaceable>nodes</replaceable>
	</term>
	<listitem>
	  <para>A list of space separated hostnames (cluster
	  members). <command>hb_report</command> may try to find out
	  the set of nodes by itself, but if it runs on the loghost
	  which, as it is usually the case, does not belong to the
	  cluster, that may be difficult. Also, OpenAIS/Corosync does
	  not contain a list of nodes and if Pacemaker is not running,
	  there is no way to find it out automatically. This option is
	  cumulative (i.e. use <option>-n</option>&nbsp;"a&nbsp;b" or
	  <option>-n</option>&nbsp;a&nbsp;<option>-n</option>&nbsp;b).</para>
	</listitem>
      </varlistentry>
      <varlistentry>
	<term>
	  <option>.l</option>&nbsp;<replaceable>file</replaceable>
	</term>
	<listitem>
	  <para>Log file location. If, for whatever reason,
	  <command>hb_report</command> cannot find the log files, you
	  can specify its absolute path.</para>
	</listitem>
      </varlistentry>
      <varlistentry>
	<term>
	  <option>-E</option>&nbsp;<replaceable>files</replaceable>
	</term>
	<listitem>
	  <para>Extra log files to collect. This option is
	  cummulative. By default,
	  <filename>/var/log/messages</filename> are collected along
	  with the cluster logs.</para>
	</listitem>
      </varlistentry>
      <varlistentry>
	<term>
	  <option>-M</option>
	</term>
	<listitem>
	  <para>Do not collect extra log files, but only the file
	  containing messages from the cluster subsystems.</para>
	</listitem>
      </varlistentry>
      <varlistentry>
	<term>
	  <option>-L</option>&nbsp;<replaceable>log-pattern</replaceable>
	</term>
	<listitem>
	  <para>A list of regular expressions to match in log files
	  for analysis. This option is additive (default: "CRIT:
	  ERROR:").</para>
	</listitem>
      </varlistentry>
      <varlistentry>
	<term>
	  <option>-p</option>&nbsp;<replaceable>variable-pattern</replaceable>
	</term>
	<listitem>
	  <para> Regular expression to match variables to be sanitized
	  (replaced by a string of asterisks). If you don’t want
	  information to be exposed in your report, you can specify
	  here a set of patterns to match variables in the CIB and
	  pengine files. stonith_host directives in ha.cf are also
	  sanitized. Logs are not sanitized. This option is
	  cummulative (default: "passw.*").</para>
	</listitem>
      </varlistentry>
      <varlistentry>
	<term>
	  <option>-A</option>
	</term>
	<listitem>
	  <para>This is an OpenAIS or Corosync cluster. hb_report has
	  some heuristics to find the cluster stack, but that is not
	  always reliable. By default, hb_report assumes that it is
	  run on a Heartbeat cluster.</para>
	</listitem>
      </varlistentry>
      <varlistentry>
	<term>
	  <option>-u</option>&nbsp;<replaceable>user</replaceable>
	</term>
	<listitem>
	  <para>The ssh user. hb_report will try to login to other
	  nodes without specifying a user, then as "root", and finally
	  as "hacluster". If you have another user for administration
	  over ssh, please use this option.</para>
	</listitem>
      </varlistentry>
      <varlistentry>
	<term>
	  <option>-S</option>
	</term>
	<listitem>
	  <para>Single node operation. Run hb_report only on this node
	  and don’t try to start slave collectors on other members of
	  the cluster. Under normal circumstances this option is not
	  needed. Use if ssh does not work to other nodes.</para>
	</listitem>
      </varlistentry>
      <varlistentry>
	<term>
	  <option>-v</option>
	</term>
	<listitem>
	  <para>Increase verbosity. Normally used to debug unexpected
	  behaviour.</para>
	</listitem>
      </varlistentry>
      <varlistentry>
	<term>
	  <option>-h</option>
	</term>
	<listitem>
	  <para>Show a brief usage message and some examples.</para>
	</listitem>
      </varlistentry>
      <varlistentry>
	<term>
	  <option>-D</option>
	</term>
	<listitem>
	  <para>(obsolete) Do not invoke editor to fill the description text file.</para>
	</listitem>
      </varlistentry>
      <varlistentry>
	<term>
	  <option>-e</option>&nbsp;<replaceable>prog</replaceable>
	</term>
	<listitem>
	  <para>(obsolete) Your favourite text editor. Defaults to
	  $EDITOR, vim, vi, emacs, or nano, whichever is found
	  first.</para>
	</listitem>
      </varlistentry>
      <varlistentry>
	<term>
	  <option>-C</option>
	</term>
	<listitem>
	  <para>Remove the destination directory once the report has
	  been put in a tarball.</para>
	</listitem>
      </varlistentry>
    </variablelist>
  </refsection>
  <refsection id="rs-hbreport-examples">
    <title>Examples</title>
    <itemizedlist>
      <listitem>
	<para>Last night, during the backup, several errors were
	logged:</para>
	<programlisting>hb_report -f 3:00 -t 4:00 -n "node1 node2" /tmp/report</programlisting>
	<para>The above invocation collects everything from all nodes
	from 3am to 4am last night. The files are compressed to a
	tarball <filename>/tmp/report.tar.bz2</filename>.</para>
      </listitem>
      <listitem>
	<para>We just encountered an issue during testing:</para>
	<programlisting># note the current time
node1# date
Fri Sep 11 18:51:40 CEST 2009
node1# @INITDIR@/heartbeat start
node1# nasty-command-that-breaks-things
node1# sleep 120 #wait for the cluster to settle
node1# hb_report -f 18:51 /tmp/hb1

# if hb_report fails to figure out that this is an OpenAIS/Corosync cluster
node1# hb_report -f 18:51 -A /tmp/hb1

# if hb_report fails to figure out the cluster members
node1# hb_report -f 18:51 -n "node1 node2" /tmp/hb1</programlisting>
          <para>The files are compressed to a tarball
        <filename>/tmp/hb1.tar.bz2</filename>.</para>
      </listitem>
    </itemizedlist>
  </refsection>
  <refsection id="rs-hbreport-interpreting">
    <title>Interpreting results</title>
    <para>The compressed tar archive is the final product of
    hb_report. This is one example of its content, for a CTS test case
    on a three node OpenAIS cluster:</para>
    <screen>
      <computeroutput>$ </computeroutput><userinput>ls -RF 001-Restart</userinput>
      <computeroutput>
001-Restart:
analysis.txt     events.txt  logd.cf       s390vm13/  s390vm16/
description.txt  ha-log.txt  openais.conf  s390vm14/

001-Restart/s390vm13:
STOPPED  crm_verify.txt  hb_uuid.txt  openais.conf@   sysinfo.txt
cib.txt  dlm_dump.txt    logd.cf@     pengine/        sysstats.txt
cib.xml  events.txt      messages     permissions.txt

001-Restart/s390vm13/pengine:
pe-input-738.bz2  pe-input-740.bz2  pe-warn-450.bz2
pe-input-739.bz2  pe-warn-449.bz2   pe-warn-451.bz2

001-Restart/s390vm14:
STOPPED  crm_verify.txt  hb_uuid.txt  openais.conf@   sysstats.txt
cib.txt  dlm_dump.txt    logd.cf@     permissions.txt
cib.xml  events.txt      messages     sysinfo.txt

001-Restart/s390vm16:
STOPPED  crm_verify.txt  hb_uuid.txt  messages        sysinfo.txt
cib.txt  dlm_dump.txt    hostcache    openais.conf@   sysstats.txt
cib.xml  events.txt      logd.cf@     permissions.txt</computeroutput></screen>
    <para>The top directory contains information which pertains to the
    cluster or event as a whole. Files with exactly the same content
    on all nodes will also be at the top, with per-node links created
    (as it is in this example the case with
    <filename>openais.conf</filename> and
    <filename>logd.cf</filename>).</para>
    <para>The cluster log files are named
    <filename>a-log.txt</filename> regardless of the actual log file
    name on the system. If it is found on the loghost, then it is
    placed in the top directory. Files named
    <filename>messages</filename> are excerpts of
    <filename>/var/log/messages</filename> from nodes.</para>
    <para>Most files are copied verbatim or they contain output of a
    command. For instance, cib.xml is a copy of the CIB found in
    <filename>@HA_VARLIBHBDIR@/crm/cib.xml</filename>. <filename>crm_verify.txt</filename>
    is the output of the
    <citerefentry><refentrytitle>crm_verify</refentrytitle><manvolnum>8</manvolnum></citerefentry>
    program.</para>
    <para>Some files are result of more involved processing:</para>
    <itemizedlist>
      <listitem>
	<formalpara>
	  <title><filename>analysis.txt</filename></title>
	  <para> A set of log messages matching user defined
	  patterns (may be provided with the <option>-L</option>
	  option).</para>
	</formalpara>
      </listitem>
      <listitem>
	<formalpara>
	  <title><filename>events.txt</filename></title>
	  <para>A set of log messages matching event patterns. It
	  should provide information about major cluster motions
	  without unnecessary details. These patterns are devised by
	  the cluster experts. Currently, the patterns cover
	  membership and quorum changes, resource starts and stops,
	  fencing (stonith) actions, and cluster starts and
	  stops. events.txt is always generated for each node. In case
	  the central cluster log was found, also combined for all
	  nodes.</para>
	</formalpara>
      </listitem>
      <listitem>
	<formalpara>
	  <title><filename>permissions.txt</filename></title>
	  <para>One of the more common problem causes are file and
	  directory permissions. <command>hb_report</command> looks
	  for a set of predefined directories and checks their
	  permissions. Any issues are reported here.</para>
	</formalpara>
      </listitem>
      <listitem>
	<formalpara>
	  <title><filename>backtraces.txt</filename></title>
	  <para>gdb generated backtrace information for cores dumped
	  within the specified period.</para>
	</formalpara>
      </listitem>
      <listitem>
	<formalpara>
	  <title><filename>sysinfo.txt</filename></title>
	  <para>Various release information about the platform,
	  kernel, operating system, packages, and anything else deemed
	  to be relevant. The static part of the system.</para>
	</formalpara>
      </listitem>
      <listitem>
	<formalpara>
	  <title><filename>sysstats.txt</filename></title>
	  <para>Output of various system commands such as ps(1),
	  uptime(1), netstat(8), and ifconfig(8). The dynamic part
	  of the system.</para>
	</formalpara>
      </listitem>
      <listitem>
	<formalpara>
	  <title><filename>description.txt</filename></title>
	  <para>Should contain a user supplied description of the
	  problem, but since it is very seldom used, it will be
	  dropped from the future releases.</para>
	</formalpara>
      </listitem>
    </itemizedlist>
  </refsection>
  <refsection>
    <title>Prerequisites</title>
    <itemizedlist>
      
      <listitem>
	<formalpara>
	  <title>ssh</title>
	  <para>It is not strictly required, but you won’t regret
	  having a password-less ssh. You may, for example, use agent
	  forwarding for this purpose. It is not too difficult to
	  setup and will save you a lot of time. If you can’t have it,
	  for example because your security policy does not allow such
	  a thing, or you just prefer menial work, then you will have
	  to resort to the semi-manual semi-automated report
	  generation. See below for instructions. If you do need to
	  supply a password for your passphrase/login, then please use
	  the <option>-u</option> option.</para>
	</formalpara>
      </listitem>
      <listitem>
	<formalpara>
	  <title>Times</title>
	  <para>In order to find files and messages in the given
	  period and to parse the -f and -t options, hb_report uses
	  perl and one of the Date::Parse or Date::Manip perl
	  modules. Note that you need only one of these. Furthermore,
	  on nodes which have no logs and where you don’t run
	  hb_report directly, no date parsing is necessary. In other
	  words, if you run this on a loghost then you don’t need
	  these perl modules on the cluster nodes. On RPM based
	  distributions, you can find Date::Parse in perl-TimeDate and
	  on Debian and its derivatives in libtimedate-perl.</para>
	</formalpara>
      </listitem>
      <listitem>
	<formalpara>
	  <title>Core dumps</title>
	  <para>To backtrace core dumps, you will need gdb and
	  debuginfo packages. The debug info packages may be installed
	  at the time the report is created. Let’s hope that you will
	  need this really seldom.</para>
	</formalpara>
      </listitem>
    </itemizedlist>
  </refsection>
  <refsection>
    <title>Times</title>
    <para>Specifying times can be a nuisance. That is why we have
    chosen to use one of the perl modules—they do allow certain
    freedom when talking dates. You can either read the instructions
    at the Date::Parse examples page. or just rely on common sense and
    try things like:</para>
    <programlisting>3:00          (today at 3am)
15:00         (today at 3pm)
2007/9/1 2pm  (September 1st at 2pm)
Tue Sep 15 20:46:27 CEST 2009 (September 15th etc)</programlisting>
    <para><command>hb_report</command> will (probably) complain if it
    cannot figure out what do you mean.</para>
    <para> Try to delimit the event as closely as possible in order to
    reduce the size of the report, but still leaving a minute or two
    around for good measure.</para>
    <para>Remember that <option>-f</option> is not optional. And don't
    forget to quote dates when they contain spaces.</para>
  </refsection>
  <refsection>
    <title>Should I send all this to the rest of the Internet?</title>
    <para>We make an effort to remove sensitive data from the
    configuration (CIB, <filename>ha.cf</filename>, and transition
    graphs). By default, parameters starting matching
    <code>passw.*</code> are replaced by a string of asterisks. Use
    the <option>-p</option> option to specify additional regular
    expressions to match variable names which may contain information
    you don't want to leak. For example:</para>
    <programlisting>hb_report -f 18:00 -p "user.*" -p "secret.*" /var/tmp/report</programlisting>
    <para>Logs and other files are not filtered.</para>
  </refsection>
  <refsection>
    <title>Logs</title>
    <para>It may be tricky to find syslog logs. The scheme used is to
    log a unique message on all nodes and then look it up in the usual
    syslog locations.  This procedure is not foolproof, in particular
    if the syslog files are in a non-standard directory. We look in
    <filename>/var/log</filename>, <filename>/var/logs</filename>,
    <filename>/var/syslog</filename>, <filename>/var/adm</filename>,
    <filename>/var/log/ha</filename>, and
    <filename>/var/log/cluster</filename>. In case we can’t find the
    logs, please supply their location:</para>
    <screen><computeroutput># </computeroutput><userinput>hb_report -f 5pm -l /var/log/cluster1/ha-log -S /tmp/report_node1</userinput></screen>
    <para>If you have different log locations on different nodes,
    well, perhaps you’d like to make them the same and make life
    easier for everybody.</para>
    <para>Files starting with <code>ha-</code> are preferred. In case
    syslog sends messages to more than one file, if one of them is
    named <filename>ha-log</filename> or
    <filename>ha-debug</filename>, then those will be favored over
    <filename>syslog</filename> or
    <filename>messages</filename>.</para>
    <para><command>hb_report</command> also supports archived logs in
    case the period specified extends that far in the past. The
    archives must reside in the same directory as the current log and
    their names must be prefixed with the name of the current log
    (such as <filename>syslog-1.gz</filename> or
    <filename>messages-20090105.bz2</filename>).
    </para>
    <para>If there is no separate log for the cluster, possibly
    unrelated messages from other programs are included. We don’t
    filter logs, just pick a segment for the period you
    specified.</para>
  </refsection>
  <refsection>
    <title>Collecting reports manually</title>
    <para>So, your ssh doesn't work. In that case, you will have to
    run this procedure on all nodes. Use <option>-S</option> so that
    we don’t bother with ssh:</para>
    <screen><computeroutput># </computeroutput><userinput>hb_report -f 5:20pm -t 5:30pm -S /tmp/report_node1</userinput></screen>
    <para>If you also have a log host which is not in the cluster,
    then you’ll have to copy the log to one of the nodes and tell us
    where it is:</para>
    <screen><computeroutput># </computeroutput><userinput>hb_report -f
    5:20pm -t 5:30pm -l /var/tmp/ha-log -S
    /tmp/report_node1</userinput></screen>
    <para>If you reconsider and want the ssh setup, take a look at the CTS <filename>README</filename> file for instructions.</para>
  </refsection>
  <refsection>
    <title>Operation</title>
    <para><command>hb_report</command> collects files and other
    information in a fairly straightforward way. The most complex
    tasks are discovering the log file locations (if syslog is used
    which is the most common case) and coordinating the operation on
    multiple nodes.</para>
    <para>The instance of <command>hb_report</command> running on the
    host where it was invoked is the master instance. Instances
    running on other nodes are slave instances.  The master instance
    communicates with slave instances by ssh. There are multiple ssh
    invocations per run, so it is essential that the ssh works without
    password, i.e. with the public key authentication and
    authorized_keys.</para>
    <para>The operation consists of three phases. Each phase must
    finish on all nodes before the next one can commence.</para>
    <para>The first phase consists of logging unique messages through
    syslog on all nodes. This is the shortest of all phases.</para>
    <para>The second phase is the most involved. During this phase all local information is collected, which includes:
    <itemizedlist>
      <listitem>
	<para>logs (both current and archived if the start time is far in the past)</para>
      </listitem>
      <listitem>
	<para>various configuration files (openais, heartbeat, logd)</para>
      </listitem>
      <listitem>
	<para>the CIB (both as xml and as represented by the crm shell)</para>
      </listitem>
      <listitem>
	<para>pengine inputs (if this node was the DC at any point in time over the given period)</para>
      </listitem>
      <listitem>
	<para>system information and status</para>
      </listitem>
      <listitem>
	<para>package information and status</para>
      </listitem>
      <listitem>
	<para>dlm lock information</para>
      </listitem>
      <listitem>
	<para>backtraces (if there were core dumps</para>
      </listitem>
    </itemizedlist>
    </para>
    <para>The third phase involves collecting information from all
    nodes and analyzing it. The analyzis consists of the following
    tasks:
    <itemizedlist>
      <listitem>
	<para>identify files equal on all nodes which may then be
	moved to the top directory</para>
      </listitem>
      <listitem>
	<para>save log messages matching user defined patterns
	(defaults to ERRORs and CRITical conditions)</para>
      </listitem>
      <listitem>
	<para>report if there were coredumps and by whom</para>
      </listitem>
      <listitem>
	<para>report
	<citerefentry><refentrytitle>crm_verify</refentrytitle><manvolnum>8</manvolnum></citerefentry>
	results</para>
      </listitem>
      <listitem>
	<para>save log messages matching major events to
	<filename>events.txt</filename></para>
      </listitem>
      <listitem>
	<para>in case logging is configured without a loghost, combine
	node logs and events files using a perl utility</para>
      </listitem>
    </itemizedlist>
    </para>
  </refsection>
  <refsection id="rs-hbreport-bugs">
    <title>Bugs</title>
    <para>Finding logs may at times be extremely difficult, depending
    on how weird the syslog configuration. It would be nice to ask
    syslog-ng developers to provide a way to find out the log
    destination based on facility and priority.</para>
    <para><command>hb_report</command> can function in a satisfactory
    way only if ssh works to all nodes using authorized_keys (without
    password).</para>
    <para>There are way too many options.</para>
  </refsection>
  <refsection id="rs-hbreport-seealso">
    <title>See also</title>
    <para>
      <citerefentry><refentrytitle>Date::Parse</refentrytitle><manvolnum>3</manvolnum></citerefentry>,
      <citerefentry><refentrytitle>heartbeat</refentrytitle><manvolnum>8</manvolnum></citerefentry>
    </para>
  </refsection>
</refentry>
